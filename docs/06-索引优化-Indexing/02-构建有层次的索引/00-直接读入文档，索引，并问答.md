### 總覽
最小可行 LlamaIndex 範例：以 `PyMuPDFReader` 讀取 PDF，直接 `VectorStoreIndex.from_documents` 建索引，使用 `as_query_engine` 進行檢索問答並顯示命中文段。

### 流程圖
```mermaid
flowchart TD
  PDF[PyMuPDFReader.load] --> DOCS[Documents]
  DOCS --> IDX[VectorStoreIndex.from_documents]
  IDX --> QE[as_query_engine(similarity_top_k=3, verbose)]
  Q[Query] --> QE --> A[回答 + source_nodes]
```

### 分步講解
- 全域設定：`Settings.llm=OpenAI(gpt-3.5)`、`Settings.embed_model=text-embedding-3-small`。
- 建索引與查詢：`VectorStoreIndex.from_documents(docs)` → `index.as_query_engine(...)`。
- 輸出：打印回答與檢索到的文本塊內容，便於觀察召回質量。

### 關鍵點總結
- **入門模板**：快速驗證檢索與回答管線是否正常。
- **可擴展**：後續可疊加重排、壓縮、分層檢索等能力。


